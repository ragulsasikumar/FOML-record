import numpy as np

class DecisionStumpRegressor:
    """
    A simple regression stump (one-level regression tree) used as the weak learner.
    """
    def __init__(self):
        self.feature_index = None
        self.threshold = None
        self.left_value = None
        self.right_value = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        best_error = float("inf")

        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])

            for t in thresholds:
                left_mask = X[:, feature] <= t
                right_mask = X[:, feature] > t

                left_value = y[left_mask].mean() if np.any(left_mask) else 0
                right_value = y[right_mask].mean() if np.any(right_mask) else 0

                preds = np.where(left_mask, left_value, right_value)
                error = np.mean((y - preds) ** 2)

                if error < best_error:
                    best_error = error
                    self.feature_index = feature
                    self.threshold = t
                    self.left_value = left_value
                    self.right_value = right_value

    def predict(self, X):
        return np.where(
            X[:, self.feature_index] <= self.threshold,
            self.left_value,
            self.right_value
        )


class GradientBoostingRegressor:
    """
    Gradient Boosting Regressor with decision stumps as weak learners.
    """
    def __init__(self, n_estimators=50, learning_rate=0.1):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.models = []
        self.initial_prediction = None

    def fit(self, X, y):
        n_samples = len(y)
        self.initial_prediction = np.mean(y)
        y_pred = np.full(n_samples, self.initial_prediction)

        for _ in range(self.n_estimators):
            residuals = y - y_pred

            stump = DecisionStumpRegressor()
            stump.fit(X, residuals)

            update = stump.predict(X)
            y_pred += self.learning_rate * update

            self.models.append(stump)

    def predict(self, X):
        y_pred = np.full(X.shape[0], self.initial_prediction)

        for stump in self.models:
            y_pred += self.learning_rate * stump.predict(X)

        return y_pred


# -----------------------------
# Example Usage
# -----------------------------
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    X, y = make_regression(n_samples=300, n_features=2, noise=15, random_state=42)

    model = GradientBoostingRegressor(n_estimators=20, learning_rate=0.1)
    model.fit(X, y)
    y_pred = model.predict(X)

    mse = np.mean((y - y_pred)**2)
    print("Training MSE:", mse)
