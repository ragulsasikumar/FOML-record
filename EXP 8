import numpy as np

class DecisionStump:
    """
    A simple decision stump used as the weak learner for AdaBoost.
    It finds the best threshold on one feature to split the data.
    """
    def __init__(self):
        self.feature_index = None
        self.threshold = None
        self.polarity = 1
        self.alpha = None

    def predict(self, X):
        n_samples = X.shape[0]
        predictions = np.ones(n_samples)
        if self.polarity == 1:
            predictions[X[:, self.feature_index] < self.threshold] = -1
        else:
            predictions[X[:, self.feature_index] >= self.threshold] = -1
        return predictions


class AdaBoost:
    """
    AdaBoost classifier using decision stumps as weak learners.
    """
    def __init__(self, n_clf=10):
        self.n_clf = n_clf
        self.clfs = []

    def fit(self, X, y):
        n_samples, n_features = X.shape
        w = np.full(n_samples, 1 / n_samples)  # Initialize weights

        for _ in range(self.n_clf):
            stump = DecisionStump()
            min_error = float("inf")

            # Find best threshold for each feature
            for feature in range(n_features):
                feature_values = X[:, feature]
                thresholds = np.unique(feature_values)

                for threshold in thresholds:
                    for polarity in [1, -1]:
                        stump.polarity = polarity
                        stump.threshold = threshold
                        stump.feature_index = feature

                        predictions = stump.predict(X)
                        error = sum(w[y != predictions])

                        if error < min_error:
                            min_error = error
                            best_stump = DecisionStump()
                            best_stump.polarity = polarity
                            best_stump.threshold = threshold
                            best_stump.feature_index = feature

            # Compute alpha (learner weight)
            EPS = 1e-10
            best_stump.alpha = 0.5 * np.log((1 - min_error) / (min_error + EPS))

            # Update weights
            predictions = best_stump.predict(X)
            w *= np.exp(-best_stump.alpha * y * predictions)
            w /= np.sum(w)

            self.clfs.append(best_stump)

    def predict(self, X):
        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]
        y_pred = np.sum(clf_preds, axis=0)
        return np.sign(y_pred)


# -----------------------------
# Example usage
# -----------------------------
if __name__ == "__main__":
    # Create simple dataset
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,
                               n_clusters_per_class=1, class_sep=1.5, random_state=42)

    # Convert labels from {0,1} to {-1, +1}
    y = np.where(y == 0, -1, 1)

    model = AdaBoost(n_clf=20)
    model.fit(X, y)
    y_pred = model.predict(X)

    accuracy = np.mean(y_pred == y)
    print("Training Accuracy:", accuracy)
